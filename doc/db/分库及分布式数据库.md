### 什么情况下需要分库
单表数据量大查询索引执行效率差的问题可以通过分区或分表的方式进行解决，分库更多是单一个库已经无法支撑需要进行扩展为几个实例。同时提高系统的可扩展性和可用性
* 单台服务器资源不足
   * CPU资源不足
   * 内存资源不足导致缓存命中率偏低(正常需要99%)
   * 磁盘资源不足
   * IO性能差
*  影响面
   * 故障影响面（16台如果一台有问题即16分之一的影响）
   * 故障恢复时间(数据量少恢复数据快)

### 总结
1. 内存问题: 由于查询时我们希望命中索引或buffer pool中能有这份数据尽可能不需要去磁盘里面查找。同时不用频繁的淘汰buffer pool中的数据。
2. cpu问题: 由于涉及到计算、连表join、group等操作可以需要很大的计算操作，同时可能会影响到其他正常的sql语句
3. 磁盘问题: 数据量已经达到了几T甚至更大，维护一个较大的磁盘开销较大。
4. IO性能差: 进行频繁的读写操作IO需要等待。

分表的解决方式就是把数据拆成N个实例。每个实例只存放一部分数据
1. 内存总体是加大的同时命中率值会比较正常(无需频繁淘汰)。
2. cup的影响面只是影响当前实例，同时由于分开可能cpu数值会变低
3. 磁盘问题因为数据较小恢复及备份等都可以以较短
4. IO操作也就没那么频繁

### 可能遇到的问题
由于数据库拆成多个实例问题：
1. 处理方案:
   * connector方案————客户端自己sql改写:
     1. 客户端需要连接多个实例，在客户端判断需要在哪个库哪个表处理(选择库及改写sql)
     2. 同时如果某个数据库实例发生了主备切换的时候，各个客户端需要同时更新(避免A客户端在主备切换时，客户端B对已经不可用的主库写数据)————dba解决方案：把主库改为readOnly，切换成功后新主库取消readOnly,切换过程无法写入数据
     3. 多实例事务问题
   * proxy层方案————proxy层sql改写:
     1. 在数据库上层加多一个proxy层，根据sql判断查询哪个实例，但可能涉及到sql的改写(客户端发过来是一个连表sql但是在不同实例上)
     2. 由于是proxy也需要高可用部署多个但是proxy的数量一定相对客户端会少得多，同时下放到proxy处理起来会更加低耦合影响面小速度更快。
     3. 多实例事务问题
     4. sql改写兼容性问题
     
2. 事务问题是最难解决的（sql改写对比起来会简单得多）靠应用程序代码来实现
同步:(强一致性)
   * 两阶段提交: 所有节点执行本地事务后不马上提交等待，则到都成功才则提交，否则回滚。
      * 由于需要等待所有节点才能提交所以会导致`阻塞等待`性能较差
      * 当有实例不满足条件时，此时的 `回滚`操作的开销就会非常的大
      * 一致性有问题第一个提交成功了，第二个还在提交，此时有读请求，`这时读到1提交的，2提交前的（脏读）` ————应用层是否看重或在应用层判断
      * 有可能当提交时发现部分提交失败(磁盘满了binlog写失败)。导致部分提交部分交失败，`提交成功无法回滚`。
      * 两阶段提交数据一致性的两个前提
        1. 假设提交阶段是可靠的(提交阶段无法回滚的结果，所以消耗尽可能短)
        2. 假设网络分区、机器崩溃或者其他原因而导致失联的节点最终能够恢复(由于在准备阶段已经写入了完整的重做日志，所以当失联机器一旦恢复，就能够从日志中找出已准备妥当但并未提交的事务数据，再向协调者查询该事务的状态，确定下一步应该进行提交还是回滚操作。)
   * 三阶段提交: 三阶段提交是对两阶段提交的改进，先对需要操作的资源先查出判断是否允许进行操作(减少不必要的事务回滚)，如果可以在执行后续的2PC
      * 与两阶段提交相同的问题，只是在CanCommit阶段会提前判断是否满足再开始事务去执行，避免不必要的回滚。同时提前访问对应的参与者可以提前检查是否因网络问题导致不可用而回滚
   * XA事务:使用两阶段提交的方式。协调者被拆成两个部分，应用（AP）和 事务管理器（TM）.
     * 将事务管理器（TM）独立成单独的组件(proxy类似)，协调事务的提交或回滚。
   * GTMS全局事务管理器:
     * 一个唯一递增的id生成器，可以生成全局事务id，可以保证每个节点的事务id是唯一的（在事务开启前SET GTID_NEXT = '12345678-1234-1234-1234-123456789012:1'; ）。
   * TCC: confirm及cancel需要考虑幂等行为（用户代码层面实现2PC）——如阿里的Seata完成
     * try阶段先把数据进行预留锁定，预留锁定成功后执行本地事务
     * 然后执行confirm阶段，把预留锁定释放掉，更新到新的业务数据中
     * 如执行cancel阶段，把预留锁定的数据还原之前的业务数据中
   * SAGA： 数据补偿来代替回滚（用户代码层面实现数据库回滚，可能会脏写需要加锁）——如阿里的Seata完成
     * 把一个大事务分解为可以交错运行的一系列子事务的集合。
     * 每个子事务都有对应的补偿动作（恢复原数据的sql）。
     * 子事务跟补偿动作都是幂等的，他们是满足交换律
   
异步:(最终一致性)
   * 本地消息表(被动): 本地事务提交后，发送消息到消息表，消息表由`定时任务`轮训去处理消息，处理成功后更新本地事务状态为成功。如果处理失败则更新本地事务状态为失败。
     * 由于需要定时任务去处理消息，延迟性会差一点，而且需要定时任务去轮训查询是否有未处理完的消息。
   * MQ方案(主动): 本地事务提交后，发送消息到`MQ`，MQ接收消息处理成功后更新本地事务状态为成功。如果处理失败则更新本地事务状态为失败。
     * 需要考虑消息丢失的问题及MQ重复消息的问题(可以通过定时任务来弥补消息丢失问题，同时幂等操作可通过分布式锁及数据id/version/status等方式解决)



### 分布式数据库
探讨什么是分布式数据库
1. 是否分布式了
2. 是否能满足ACID

上云后觉得这些就应该让云厂商去做。让专业的人做效率及准确性更高。目前云厂商给出的方案:
1. shared nothing: 方案最成熟，各个节点处理自己的事，处理跨节点事务
2. raft同步: 需要同步，更新半数同意原则（最贴近分布式去中心化的思想，但是节点越多raft同步的效率越差,该方案也可以去掉proxy层请求可内部转发）
3. 存算分离: 优势扩展时只需增加server节点，不用考虑数据节点

ACID与CAP理论
* 传统数据库应该满足ACID(AID是手段，C是目的)
  * Atomic: 原子性 
  * Isolation: 隔离性
  * Durability: 持久性
  * Consistency: 一致性
* 由于分布式存在多副本的问题那就引出CAP理论，CP:强一致性(关系型数据库)  AP:最终一致性(NOSQL数据库)
  * P分区容忍性: 当部分节点因网络原因而彼此失联系统仍能正确地提供服务的能力
  * C一致性: 任何时刻、任何分布式节点中，我们所看到的数据都是没有矛盾的（因网络原因同步不完成，需等到全部同步完才能提供服务）
  * P可用性: 任何时刻都能访问到分布式中的数据（可以牺牲一致性，因网络原因同步不完成，依旧可以提供服务只是数据时旧数据）

跨库事务的方案
1. proxy方案(适用写多同时有share key情况): 修改sql
   * 广播表（小表），更新是更新到每个库，每次更新都广播到各个实例
   * `涉及到多实例处理时需要改写sql`
   * 查询/更新各个实例的数据在proxy层汇总处理或某台实例汇总处理
   * 一致性读的问题通过GTMS方案解决
2. raft方案(适用查多写少): 
   * 节点会通过raft算法同步其他节点的部分数据(该方案无主备集群。把备数据放到其他节点上)
   * 查询时如果当前节点有全部的数据则可直接查找返回，如果无其他数据可向其他节点获取后再本地合并(temporary table)
   * `更新数据时，多半同意机制（如果小于一半回滚的开销也很大）`
   * 一致性读的问题通过GTMS方案解决
3. 存算分离方案:
   * 所有实现在引擎层处理这个需要看各个云商家的处理


## 腾讯云TDSQL