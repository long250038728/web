### 什么情况下需要分库
单表数据量大查询索引执行效率差的问题可以通过分区或分表的方式进行解决，分库更多是单一个库已经无法支撑需要进行扩展为几个实例。同时提高系统的可扩展性和可用性
* 单台服务器资源不足
   * CPU资源不足
   * 内存资源不足导致缓存命中率偏低(正常需要99%)
   * 磁盘资源不足
   * IO性能差
*  影响面
   * 故障影响面（16台如果一台有问题即16分之一的影响）
   * 故障恢复时间(数据量少恢复数据快)

### 总结
1. 内存问题: 由于查询时我们希望命中索引或buffer pool中能有这份数据尽可能不需要去磁盘里面查找。同时不用频繁的淘汰buffer pool中的数据。
2. cpu问题: 由于涉及到计算、连表join、group等操作可以需要很大的计算操作，同时可能会影响到其他正常的sql语句
3. 磁盘问题: 数据量已经达到了几T甚至更大，维护一个较大的磁盘开销较大。
4. IO性能差: 进行频繁的读写操作IO需要等待。

分表的解决方式就是把数据拆成N个实例。每个实例只存放一部分数据
1. 内存总体是加大的同时命中率值会比较正常(无需频繁淘汰)。
2. cup的影响面只是影响当前实例，同时由于分开可能cpu数值会变低
3. 磁盘问题因为数据较小恢复及备份等都可以以较短
4. IO操作也就没那么频繁

### 可能遇到的问题
由于数据库拆成多个实例问题：
1. 处理方案:
   * connector方案————客户端自己sql改写:
     1. 客户端需要连接多个实例，在客户端判断需要在哪个库哪个表处理(选择库及改写sql)
     2. 同时如果某个数据库实例发生了主备切换的时候，各个客户端需要同时更新(避免A客户端在主备切换时，客户端B对已经不可用的主库写数据)————dba解决方案：把主库改为readOnly，切换成功后新主库取消readOnly,切换过程无法写入数据
     3. 多实例事务问题
   * proxy层方案————proxy层sql改写:
     1. 在数据库上层加多一个proxy层，根据sql判断查询哪个实例，但可能涉及到sql的改写(客户端发过来是一个连表sql但是在不同实例上)
     2. 由于是proxy也需要高可用部署多个但是proxy的数量一定相对客户端会少得多，同时下放到proxy处理起来会更加低耦合影响面小速度更快。
     3. 多实例事务问题
     4. sql改写兼容性问题
2. 事务问题是最难解决的（sql改写对比起来会简单得多）
   * 两阶段提交: search
      * 一致性有问题第一个提交成功了，第二个还在提交，此时有读请求，这时读到1提交的，2提交前的（脏读）—————— 看是否校验
      * 两个事务执行后不提交，所有执行成功后都没问题再提交。或都一起回滚。(只有一个时候有问题(但不考虑磁盘空间不够)，binlog写失败。导致一个可以提交一个提交失败，提交成功无法回滚)
   * TCC: search
   * GTMS全局事务管理器: search

### 分布式数据库
探讨什么是分布式数据库
1. 是否分布式了
2. 是否能满足ACID

上云后觉得这些就应该让云厂商去做。让专业的人做效率及准确性更高。目前云厂商给出的方案:
1. shared nothing: 方案最成熟，各个节点处理自己的事，处理跨节点事务
2. raft同步: 需要同步，更新半数同意原则（最贴近分布式去中心化的思想，但是节点越多raft同步的效率越差,该方案也可以去掉proxy层请求可内部转发）
3. 存算分离: 优势扩展时只需增加server节点，不用考虑数据节点

跨库事务的方案
1. proxy方案(适用写多同时有share key情况): 修改sql
   * 广播表（小表），更新是更新到每个库，每次更新都广播到各个实例
   * __涉及到多实例处理时需要改写sql__
   * 查询/更新各个实例的数据在proxy层汇总处理或某台实例汇总处理
   * 一致性读的问题通过GTMS方案解决
2. raft方案(适用查多写少): 
   * 节点会通过raft算法同步其他节点的部分数据(该方案无主备集群。把备数据放到其他节点上)
   * 查询时如果当前节点有全部的数据则可直接查找返回，如果无其他数据可向其他节点获取后再本地合并(temporary table)
   * __更新数据时，多半同意机制（如果小于一半回滚的开销也很大）__
   * 一致性读的问题通过GTMS方案解决
3. 存算分离方案:
   * 所有实现在引擎层处理这个需要看各个云商家的处理




<!-- 需要查询的内容 -->
冷热数据（查询一下es冷热集群）
存储存放到oss里面
show detail processlist;





生成全局事务id，告诉每个节点的事务id，备份读取时取一个老一点的事务id就能保证每个节点在某个节点的视图（每个节点加一个读锁事务提交，然后读的时候是这个读的这个事务id时的视图）。










