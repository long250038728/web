## 目录
- [什么情况下需要分库](#什么情况下需要分库)
- [总结](#总结)
- [可能遇到的问题](#可能遇到的问题)
  - [如何实现](#如何实现)
  - [事务问题](#事务问题)
- [分布式数据库](#分布式数据库)
  - [一致性](#一致性)
    - [事务一致性](#事务一致性)
    - [数据一致性](#数据一致性)
  - [分布式数据库的类型](#分布式数据库的类型)
  - [分布式数据库局限](#分布式数据库局限)
  - [计算与存储分离优化方案](#计算与存储分离优化方案)
- [公有云分布式数据库](#公有云分布式数据库)
  - [跨库事务的方案](#跨库事务的方案)
- [分布式一致算法](#分布式一致算法)
  - [Paxos算法](#Paxos算法)
    - [Multi Paxos算法](#Multi Paxos算法)
  - [Raft算法](#Raft算法)
  - [Gossip算法](#Gossip算法)
- [补充](#补充)
---

### 什么情况下需要分库
单表数据量大查询索引执行效率差的问题可以通过分区或分表的方式进行解决，分库更多是单一个库已经无法支撑需要进行扩展为几个实例。同时提高系统的可扩展性和可用性
* 单台服务器资源不足
   * CPU资源不足
   * 内存资源不足导致缓存命中率偏低(正常需要99%)
   * 磁盘资源不足
   * IO性能差
*  影响面
   * 故障影响面（16台如果一台有问题即16分之一的影响）
   * 故障恢复时间(数据量少恢复数据快)

### 总结
遇到需要分库的问题：
1. 内存问题: 由于查询时我们希望命中索引或buffer pool中能有这份数据尽可能不需要去磁盘里面查找。同时不用频繁的淘汰buffer pool中的数据。
2. cpu问题: 由于涉及到计算、连表join、group等操作可以需要很大的计算操作，同时可能会影响到其他正常的sql语句
3. 磁盘问题: 数据量已经达到了几T甚至更大，维护一个较大的磁盘开销较大。
4. IO性能差: 进行频繁的读写操作IO需要等待。

分库的解决方式就是把数据拆成N个实例。每个实例只存放一部分数据：
1. 内存总体是加大的同时命中率值会比较正常(无需频繁淘汰)。
2. cup的影响面只是影响当前实例，同时由于分开可能cpu数值会变低
3. 磁盘问题因为数据较小恢复及备份等都可以以较短
4. IO操作也就没那么频繁

### 可能遇到的问题
#### 如何实现:
   * connector方案————客户端自己sql改写:
     1. 客户端需要连接多个实例，在客户端判断需要在哪个库哪个表处理(选择库及改写sql)
     2. 同时如果某个数据库实例发生了主备切换的时候，各个客户端需要同时更新(避免A客户端在主备切换时，客户端B对已经不可用的主库写数据)————dba解决方案：把主库改为readOnly，切换成功后新主库取消readOnly,切换过程无法写入数据
     3. 多实例事务问题
   * proxy层方案————proxy层sql改写:
     1. 在数据库上层加多一个proxy层，根据sql判断查询哪个实例，但可能涉及到sql的改写(客户端发过来是一个连表sql但是在不同实例上)
     2. 由于是proxy也需要高可用部署多个但是proxy的数量一定相对客户端会少得多，同时下放到proxy处理起来会更加低耦合影响面小速度更快。
     3. 多实例事务问题
     4. sql改写兼容性问题
     
#### 事务问题
同步处理:(强一致性)
   * 两阶段提交: 所有节点执行本地事务后不马上提交等待，则到都成功才则提交，否则回滚。
      * 由于需要等待所有节点才能提交所以会导致`阻塞等待`性能较差
      * 当有实例不满足条件时，此时的 `回滚`操作的开销就会非常的大
      * 一致性有问题第一个提交成功了，第二个还在提交，此时有读请求，`这时读到1提交的，2提交前的（脏读）` ————应用层是否看重或在应用层判断
      * 有可能当提交时发现部分提交失败(磁盘满了binlog写失败)。导致部分提交部分交失败，`提交成功无法回滚`。
      * 两阶段提交数据一致性的两个前提
        1. 假设提交阶段是可靠的(提交阶段无法回滚的结果，所以消耗尽可能短)
        2. 假设网络分区、机器崩溃或者其他原因而导致失联的节点最终能够恢复(由于在准备阶段已经写入了完整的重做日志，所以当失联机器一旦恢复，就能够从日志中找出已准备妥当但并未提交的事务数据，再向协调者查询该事务的状态，确定下一步应该进行提交还是回滚操作。)
   * 三阶段提交: 三阶段提交是对两阶段提交的改进，先对需要操作的资源先查出判断是否允许进行操作(减少不必要的事务回滚)，如果可以在执行后续的2PC
      * 与两阶段提交相同的问题，只是在CanCommit阶段会提前判断是否满足再开始事务去执行，避免不必要地回滚。同时提前访问对应的参与者可以提前检查是否因网络问题导致不可用而回滚
   * XA事务:使用两阶段提交的方式。协调者被拆成两个部分，应用（AP）和 事务管理器（TM）.
     * 将事务管理器（TM）独立成单独的组件(proxy类似)，协调事务的提交或回滚。
   * GTMS全局事务管理器:
     * 一个唯一递增的id生成器，可以生成全局事务id，可以保证每个节点的事务id是唯一的（在事务开启前SET GTID_NEXT = '12345678-1234-1234-1234-123456789012:1'; ）。
   * TCC: confirm及cancel需要考虑幂等行为（用户代码层面实现2PC）——如阿里的`Seata`完成
     * try阶段先把数据进行预留锁定，预留锁定成功后执行本地事务
     * 然后执行confirm阶段，把预留锁定释放掉，更新到新的业务数据中
     * 如执行cancel阶段，把预留锁定的数据还原之前的业务数据中
   * SAGA： 数据补偿来代替回滚（`用户代码层面`实现数据库回滚，可能会脏写需要加锁）——如阿里的`Seata`完成
     * 把一个大事务分解为可以交错运行的一系列子事务的集合。
     * 每个子事务都有对应的补偿动作（恢复原数据的sql）。
     * 子事务跟补偿动作都是幂等的，他们是满足交换律
     * 执行时在数据库是以提交的方式进行避免数据库锁及死锁的问题，回滚时是通过另外语句进行回滚。所以中间就会导致脏写及脏读问题
   * AT： 数据补偿来代替回滚（在`代理层`根据sql会生成undo log的sql用于回滚，由于是代理生成所以SQL兼容性可能会差，可能会脏写需要加锁）——如阿里的`Seata`完成
       * 与SAGA相同，只是生成的回滚sql是由代理层实现，SQL兼容性差但是对回滚透明

XA事务
```SQL
-- 启动XA事务
XA START 'xxxxxx11111';
UPDATE `server` SET `group` = "ari" WHERE `group` = "a";
-- 结束XA事务分支
XA END 'xxxxxx11111';
-- 准备XA事务
XA PREPARE 'xxxxxx11111';

-- 回滚XA事务
XA ROLLBACK 'xxxxxx11111';
-- 提交XA事务
XA COMMIT 'xxxxxx11111';
```
      
   
异步处理:(最终一致性)
   * 本地消息表(被动): 本地事务提交后，发送消息到消息表，消息表由`定时任务`轮训去处理消息，处理成功后更新本地事务状态为成功。如果处理失败则更新本地事务状态为失败。
     * 由于需要定时任务去处理消息，延迟性会差一点，而且需要定时任务去轮训查询是否有未处理完的消息。
   * MQ方案(主动): 本地事务提交后，发送消息到`MQ`，MQ接收消息处理成功后更新本地事务状态为成功。如果处理失败则更新本地事务状态为失败。
     * 需要考虑消息丢失的问题及MQ重复消息的问题(可以通过定时任务来弥补消息丢失问题，同时幂等操作可通过分布式锁及数据id/version/status等方式解决)



---
## 分布式数据库
写多读少，事务，高可用，分布式，高并发，海量存储

### 一致性
#### 事务一致性
* `ACID` 常规关系型数据库 
  * Consistency一致性是目的  Atomic原子性 Isolation隔离性 Durability持久性 是手段 ———C隔离性是最难的
  * Consistency隔离性使用MVCC进行,通过undo Log进行回滚查看，事务提交后如果不需要会进行清除
* `BASE` 分布式数据库  
  * BA基本可用性  S数据处理中可能各个节点看到的不一致  E最终一致性
* `CAP` 分布式理论(由于分布式存在多副本的问题那就引出CAP理论，CP:强一致性(牺牲高可用)  AP:最终一致性(牺牲一致性))
    * P分区容忍性: 当部分节点因网络原因而彼此失联系统仍能正确地提供服务的能力
    * C一致性: 任何时刻、任何分布式节点中，我们所看到的数据都是没有矛盾的（因网络原因同步不完成，需等到全部同步完才能提供服务）
    * A可用性: 任何时刻都能访问到分布式中的数据（可以牺牲一致性，因网络原因同步不完成，依旧可以提供服务只是数据时旧数据）
  

#### 数据一致性
多个副本数据一致性)    
* 强一致：所有的副本都返回成功才算成功 （性能差,可用性差）
* 弱一致：最终一致性
  1. 写副本A然后读副本B   (B落后A——使用写读同个副本解决)
  2. 读副本B后读副本A   (B落后A——使用同个设备/人同个副本解决)
  3. 写副本A然后写副本B,读副本C时顺序错乱 (两个主，同步的顺序——使用全局时钟解决)


### 分布式数据库的类型
* `Agent`: 在Agent(协调节点)进行转发，SQL重写,多节点的查询，多节点联表，多节点事务，多节点数据汇总排序等
* `NewSQL`: 原生数据库，基础是NoSQL(分片，Paxos/Raft共识,LSM-Tree等)
  * 支持`动态分片`，可以根据分片的请求压力切换到不同的节点上保证节点之间的负载是均衡的。 同时可以把事务内的控制在同一个节点上。
  * 有单独的`元数据集群`进行保存，同时通过同步协议进行同步
  * 由于是NewSQL重新实现了数据库的内部逻辑，所以在OLTP上进行了扩展`HATP`两个存储结构引擎（混合OLAP分析及OLTP事务系统）——Agent这一点就无法实现只能通过ETL方式
    * OLTP 使用行式存储  （写快，单行读快 多行读慢）
    * OLAP 使用列式存储/全文检索等  （写慢(需要写多个数据页-使用LSM-Tree来优化)，单行读慢，同列多行快）

### 分布式数据库局限
* 不使用`存储过程`（移植性差，调试难）——数据在多个节点及分片原因存储过程会变得更加复杂(不常用所以产商不深挖)
* 不使用`自增主键` (唯一性，单调递增 都不能满足) ——  分布式生成需要考虑中心化分配id问题（高并发性能影响）
  * 雪花算法：     
    1. 局部有序(随机的高位(前51位)使得数据在分片中分布更加均衡，避免尾部效应)
    2. 分散分布式数据库id生成压力(解决单点瓶颈及锁)  
    3. 当使用range分片时会导致数据某个分区压力过大(雪花算法前面的机器码可以分散到其他分片)
       * 数据库定义为BIGINT: 1位符号位(固定为0)  41位时间戳(毫秒)  10位机器码(最大1024个机器)  12位序列号(最大1毫秒生成4096个id)
  * UUID：
    1. 无序性(避免分布式数据库单点压力大的问题)
    2. 索引效率低(单分片下无序导致性能较差)

### 计算与存储分离优化方案
计算节点与存储节点之间没有无效的数据
* `计算下推`到存储节点
    * TiDB采用“缓存写提交”则将写的SQL缓存起来，直到事务提交再一起发到存储节点。即在同个事务中执行insert后执行select时，insert还没有到存储节点。后续改进为将数据节点跟本地缓存进行merge合并得到真实数据(与刷脏页逻辑相同)
* `分区键`
    * 可以根据分区间下推到各个存储节点，同时多表关联也是使用相同的下推方式。以确保数据移动最少
* `索引`
    * 全局索引： 全局索引是为了检验唯一索引的值是否有冲突(如果去各个分区索引去查找效率较差)，但是维护这个全局索引需要加上分布式锁等代价
    * 分区索引： 索引是为了可以快速地查找对应的数据是否满足而不用遍历全部进行查询，当分区的数据分裂合并时对应的分区索引也要发生改变
* `JOIN下推`
    * 大表关联`小表`
        1. 静态(每次更新时各个节点都处理)：定义为每个节点都有的广播表，执行join时大表所在节点join小表
        2. 动态(有需要时再同步到广播到各个节点)：执行时把小表的数据广播到每个对应的节点，执行join时大表所在节点join小表
    * 大表关联`大表`
        1. 重分布： 在多个节点上做本地关联合并
            * 关联时如果指定了A的分区键，没有B的分区键，那么把B的数据推送到A的各个分片上进行本地管理(明确了A的所在的分区，把B的数据推送到A上面去)
            * 关联时如果都不是AB表的分区键，则将A跟B的数据推送到各个节点（所有节点都有A跟B的数据，所有节点联表后，到统一去重）
* `LSM-Tree`
  * 流程
    1. 写入时先将数据写入内存结构中(Memtable)&&WAL预写日志避免宕机数据丢失,当达到一定的设定值后会写入日志文件中(Tablet log),形成有序的文件(SSTable)
    2. 周期性的会将多个SSTable文件合并成一个大的SSTable(Compact操作)，目的是为了减少SSTable的数量，同时起到合并数据及减少加载SSTable的次数
       * 但是在合并过程中需要产生新的SSTable空间合并后删除旧的SSTable，对磁盘空间有影响
       * 同时需要有后台任务进行合并，此时会读放大。对CPU及内存有影响
    3. 读请求时先从内存结构中(memtable)获取，如果找不到从磁盘上的SSTable中查找
  * 合并策略( 周期性触发的，可以基于时间、SSTable数量、大小等因素)
    * Tiered: 将所有的SSTable合并，性能差
    * Leveled: 分层的概念将数据分成一些列Key互不重叠且固定大小的SSTable，每次Compact只处理少量SSTable文件，性能好
  * 与mysql的区别
    1. B+树刷脏页时是先加载到内存中然后进行合并数据在刷到磁盘(内存+磁盘)，优化读操作 , 
    2. LSM-Tree是把SSTable进行合并数据(磁盘+磁盘)，优化写操作, 
    3. B+树每次数据变动都会更新树结构
    4. LSM-Tree 以局部有序为概念，Memtable有序，SSTable内部有序。只有才Compact后SSTable才会全局有序 (读性能较差，所以不适合OLTP)
    5. LSM-Tree 使用Bloom Filters等技术来优化，减少需要检查的SSTable数量
* `Raft`
  * 由于Raft一致性算法需要半数节点应答才能算成功，同时是由主提案节点进行提案，所以不适合用写多的场景。
  * 当Mysql主从同步设置为半同步时确实会比Raft性能强，但只有设置了半同步的那节点数据一致性，其他异步的就不能保证一致性
  * 当Mysql主从同步设置为全同步时需要全部从节点确认后才能提交，Raft则是过半可减少一半的时间
  * Mysql从节点宕机后恢复，可根据`GTID`和`binlog`进行恢复（由于`binlog`有过期删除机制,如果获取到的binlog已经不存在(错误日志会报错)，则需要通过加载全量备份后再增量binlog恢复）
  * Raft从节点宕机后恢复，会判断日志差距大小，如果过大则会发送快照。 如果小于阈值会发送日志同步
  * Raft一致性算法跟Mysql的同步机制都是各有优点，分布式系统一致性优先所以一般会使用Raft，OLTP是性能优先所以使用主从同步
  * Mysql是性能优先(OLTP不使用Raft的原因)，Raft是一致性优先。
---

## 公有云分布式数据库
上云后觉得这些就应该让云厂商去做。让专业的人做效率及准确性更高。目前云厂商给出的方案:
1. shared nothing: 方案最成熟，各个节点处理自己的事，处理跨节点事务
2. raft同步: 需要同步，更新半数同意原则（最贴近分布式去中心化的思想，但是节点越多raft同步的效率越差,该方案也可以去掉proxy层请求可内部转发）
3. 存算分离: 优势扩展时只需增加server节点，不用考虑数据节点

探讨是是不是分布式数据库
1. 是否分布式了
2. 是否能满足ACID


### 跨库事务的方案
1. proxy方案(适用写多同时有share key情况): 修改sql
   * 广播表（小表），更新是更新到每个库，每次更新都广播到各个实例
   * `涉及到多实例处理时需要改写sql`
   * 查询/更新各个实例的数据在proxy层汇总处理或某台实例汇总处理
   * 一致性读的问题通过GTMS方案解决
2. raft方案(适用查多写少): 
   * 节点会通过raft算法同步其他节点的部分数据(该方案无主备集群。把备数据放到其他节点上)
   * 查询时如果当前节点有全部的数据则可直接查找返回，如果无其他数据可向其他节点获取后再本地合并(temporary table)
   * `更新数据时，多半同意机制（如果小于一半回滚的开销也很大）`
   * 一致性读的问题通过GTMS方案解决
3. 存算分离方案:
   * 所有实现在引擎层处理这个需要看云商家的处理
   * 由于是存算分离，那么在server层可能有多个，此时写操作命中A服务，其他BCD..等的buffer_pool可能命中不到
 
---

## 分布式一致算法
在软件系统中为了保证数据可用不丢失就需要通过多台备份机制进行保存。由于保存的是一份随时变动的数据。所以数据同步需要深入思考。
* 状态转移:  所有节点保存真实数据值，依赖对快照或完整数据副本的同步
* 操作转移:  需要确保每个节点以相同的顺序执行操作(所有节点初始值是一样的状态，执行命令序列后，最终的状态一定一致)

网络分区的现象是无法消除的，所以无法追求所有节点任何时候状态一致，采用少数服从多数的原则(n/2+1)。让网络分区，机器崩溃等原因不影响正常写入，恢复后可追上数据一致。

### Paxos算法
分为三个节点
* 提案节点: 提出对某个值的操作节点
* 决策节点:  应答提案节点参与投票，接受或反对
* 记录节点:  不参与决策(一般是从网络分区恢复中数据还没跟上同步的节点)

分布式数据一致性的复杂度，主要下面的因素影响
* 由于网络问题可能导致延迟，同时多节点并发问题，如果不等待/加锁加载顺序不一致导致数据最终不一致。但如果加锁在锁释放前发生崩溃失联导致后续操作阻塞。
* 由于网络问题可能丢失

#### 提案流程
* 准备阶段： 如果提案节点发出提案，必须向所有决策节点广播申请许可。会发一个全局唯一的id为提案号(递增)。格式是Prepare(id,value),广播所有决策节点。
    1. 两个承诺：承诺不会再`接受`提案id小于或等于的`准备请求`。承诺不在`批准`小于n的`批准请求` （站在决策节点的角度明确自己是最新的才允许去参与投票）
        * 如果再接收到小于或等于的准备请求(别人提案节点网络不好)
        * 如果再接收到小于的批准请求(自己决策节点网络不好， 因为有新的提案请求代表之前的批准流程已经结束)
    2. 一个应答：回复提案节点批准过最大的提案id及值(本地无传空)。如果收到的id不是决策节点最大的则不会理会这个准备请求
        * 如果提案节点等到超时时间还没有超过 n/2 +1 的人答复则重新发起新的提案(提案递增)，超时次数到达时则认为提案失败

* 批准阶段:
    1.  提案节点发现所有响应的节点返回的值是空，代表自己是第一个设置值的节点，可以随意改，发起Accept(id,value)再次广播全部的决策的节点, 每一个决策节点收到Accept后返回应答
    2. 如果Prepare(id,value)返回响应的中有值，必须无条件从应答中找到提案最大的id对应的值，发起Accept(maxAcceptID,maxAcceptValue)，再次广播全部的决策的节点, 如果接收到的Accept是自己最大则同意，就会认为Prepare(id,value)的值是可以批准的。则返回应答
    3. 如果提案节点等到超时时间还没有超过 n/2 +1 的人答复则重新发起新的提案，超时次数到达时则认为提案失败

### Multi Paxos算法
通过增加`选主`的过程
1. 提案节点通过定时轮训(心跳)，确定当前网络所有节点是否存在一个`主提案节点`
2. 如果没有会通过提案，批准两轮网络交互选出`主提案节点`(即享所有节点广播希望自己竞选主节点的请求，如果得到多数批准则选主成功）
   * 前提: 1.超半节点认同无主提案节点  2.提案靠前的节点成功主的可能性更强(需要符合两个承诺，不在接受小于或等于N的准备请求，不再接受小于N的批准请求)

提案方式变为无并发的有序操作
1. 只有主节点才能提案
2. 跟随节点收到提案后会转发到主节点由主节点提案

### Raft算法
Raft 是 Multi-Paxos 的一种变体，其设计目标是简化并更易于实现 Multi-Paxos，同时解决实际应用中的一些复杂性问题。
* 网络分区后恢复后的处理
  1. 选出主提案节点
  2. 每个节点会比较它的日志条目与其他节点的日志条目
  3. 如果某个节点的日志条目与新选出的领导者的日志不一致，领导者会要求该节点回滚到一个共同的日志点，并从该点重新同步日志
  4. 最终只会有一个分区的日志被保留，其他分区的日志会被删除（领导者会比较两个日志条目，并通过`日志回滚`和`重同步`确保一致性。如果分区2的节点数较多，那么`日志条目B`会被保留，分区1中的节点会回滚并同步日志。）

### Gossip算法
这是一个`最终一致性`算法。通过`流传病算法`方式
1. 只是随机广播相邻连接的几个节点，然后再以此类推广播相邻连接的几个节点直到全部的节点都收到了数据

## 补充
1. 部署了3台数据库。可以对一个数据进行三副本的部署。
   * 表1的主副本leader，在数据库1节点上
   * 表1的跟随副本Follower，在数据库2节点上
   * 表1的跟随副本Follower，在数据库3节点上
2. 读数据时
   * 根据proxy节点通过心跳检测指向正常的某台数据库节点（需要指向没有失联的数据库节点！！）
   * 此时读数据时可以对跟随副本进行读取（有可能读到的是没有投票的节点，此时数据可能是落后的），也可以对主副本进行读取。
   * 此时如果读的是多个表，
     1. 多个表在一个数据库节点上，此时可以同时对多个表进行读取。
     2. 多个表不在同一个数据库节点上，可以进行转发获取其他数据库节点上的数据。然后当前数据库节点汇总
3. 写数据时（元数据管理机制管理——可以是单独一个服务保存，可以保存在各个数据节点上）
   * 写数据时会找到数据库对应的表的leader节点，此时写数据时会对其他数据库节点的跟随者发起投票，超过一半应答代表成功，则写成功（为了解决部分节点失联无法应答的问题）
4. 部分数据库节点失联
   * 当数据库失联时
     1. 主副本数据库失联，跟随副本会进行投票投出新的主副本，此时失联的主副本虽然自己认为是主副本但是每次投票都少于一半的应答，所以写入不会成功。当与其他数据库节点连接上后，发现自己落后别人，此时进行合并数据同时自己降级为跟随节点
     2. 跟随副本数据库失联。此处由于所有的投票的竞选都收不到，所以起不到任何作用。恢复后，基于日志的回放同步数据。


### 腾讯云服务
1. TDSQL-Innodb版
   * 采用最原始的Innodb版本.把表数据通过share key分散到不同的节点上。(分区表，广播表, 全局表)
2. TDSQL-Store版本
   * 采用Raft及LSM-Tree分布式协调算法，把表数据通过分片的方式存储在各个节点上。分为主副本及跟随副本。主副本提供读写，跟随副本提供读。
   * 写操作时，通过Raft中的主副本进行发起写操作，然后通过Raft中的跟随副本进行投票。超过半数的应答代表成功，则写入成功。
   * 当某台服务器进行宕机时，该服务的主副本相关的分区会进行投票选主
3. TDSQL-C版本
   * 采用存算分离的架构。让多个计算节点共享一份数据.
   * 该架构下binlog好像就并不是特别重要。
   * 一个集群中仅支持一个读写实例
   * 分布式为了解决数据的存储问题及计算问题。
     * 存储：用一个很厉害的存储系统存储数据不用考虑之间的同步问题。解决了事务的一致性问题。
     * 计算节点：计算节点由于无状态/少状态(元数据)的特性，在扩展计算节点只需同步很少的数据信息就好