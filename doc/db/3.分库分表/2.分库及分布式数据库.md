### 什么情况下需要分库
单表数据量大查询索引执行效率差的问题可以通过分区或分表的方式进行解决，分库更多是单一个库已经无法支撑需要进行扩展为几个实例。同时提高系统的可扩展性和可用性
* 单台服务器资源不足
   * CPU资源不足
   * 内存资源不足导致缓存命中率偏低(正常需要99%)
   * 磁盘资源不足
   * IO性能差
*  影响面
   * 故障影响面（16台如果一台有问题即16分之一的影响）
   * 故障恢复时间(数据量少恢复数据快)

### 总结
1. 内存问题: 由于查询时我们希望命中索引或buffer pool中能有这份数据尽可能不需要去磁盘里面查找。同时不用频繁的淘汰buffer pool中的数据。
2. cpu问题: 由于涉及到计算、连表join、group等操作可以需要很大的计算操作，同时可能会影响到其他正常的sql语句
3. 磁盘问题: 数据量已经达到了几T甚至更大，维护一个较大的磁盘开销较大。
4. IO性能差: 进行频繁的读写操作IO需要等待。

分库的解决方式就是把数据拆成N个实例。每个实例只存放一部分数据
1. 内存总体是加大的同时命中率值会比较正常(无需频繁淘汰)。
2. cup的影响面只是影响当前实例，同时由于分开可能cpu数值会变低
3. 磁盘问题因为数据较小恢复及备份等都可以以较短
4. IO操作也就没那么频繁

### 可能遇到的问题
由于数据库拆成多个实例问题：
1. 处理方案:
   * connector方案————客户端自己sql改写:
     1. 客户端需要连接多个实例，在客户端判断需要在哪个库哪个表处理(选择库及改写sql)
     2. 同时如果某个数据库实例发生了主备切换的时候，各个客户端需要同时更新(避免A客户端在主备切换时，客户端B对已经不可用的主库写数据)————dba解决方案：把主库改为readOnly，切换成功后新主库取消readOnly,切换过程无法写入数据
     3. 多实例事务问题
   * proxy层方案————proxy层sql改写:
     1. 在数据库上层加多一个proxy层，根据sql判断查询哪个实例，但可能涉及到sql的改写(客户端发过来是一个连表sql但是在不同实例上)
     2. 由于是proxy也需要高可用部署多个但是proxy的数量一定相对客户端会少得多，同时下放到proxy处理起来会更加低耦合影响面小速度更快。
     3. 多实例事务问题
     4. sql改写兼容性问题
     
2. 事务问题是最难解决的（sql改写对比起来会简单得多）靠应用程序代码来实现
同步:(强一致性)
   * 两阶段提交: 所有节点执行本地事务后不马上提交等待，则到都成功才则提交，否则回滚。
      * 由于需要等待所有节点才能提交所以会导致`阻塞等待`性能较差
      * 当有实例不满足条件时，此时的 `回滚`操作的开销就会非常的大
      * 一致性有问题第一个提交成功了，第二个还在提交，此时有读请求，`这时读到1提交的，2提交前的（脏读）` ————应用层是否看重或在应用层判断
      * 有可能当提交时发现部分提交失败(磁盘满了binlog写失败)。导致部分提交部分交失败，`提交成功无法回滚`。
      * 两阶段提交数据一致性的两个前提
        1. 假设提交阶段是可靠的(提交阶段无法回滚的结果，所以消耗尽可能短)
        2. 假设网络分区、机器崩溃或者其他原因而导致失联的节点最终能够恢复(由于在准备阶段已经写入了完整的重做日志，所以当失联机器一旦恢复，就能够从日志中找出已准备妥当但并未提交的事务数据，再向协调者查询该事务的状态，确定下一步应该进行提交还是回滚操作。)
   * 三阶段提交: 三阶段提交是对两阶段提交的改进，先对需要操作的资源先查出判断是否允许进行操作(减少不必要的事务回滚)，如果可以在执行后续的2PC
      * 与两阶段提交相同的问题，只是在CanCommit阶段会提前判断是否满足再开始事务去执行，避免不必要的回滚。同时提前访问对应的参与者可以提前检查是否因网络问题导致不可用而回滚
   * XA事务:使用两阶段提交的方式。协调者被拆成两个部分，应用（AP）和 事务管理器（TM）.
     * 将事务管理器（TM）独立成单独的组件(proxy类似)，协调事务的提交或回滚。
   * GTMS全局事务管理器:
     * 一个唯一递增的id生成器，可以生成全局事务id，可以保证每个节点的事务id是唯一的（在事务开启前SET GTID_NEXT = '12345678-1234-1234-1234-123456789012:1'; ）。
   * TCC: confirm及cancel需要考虑幂等行为（用户代码层面实现2PC）——如阿里的`Seata`完成
     * try阶段先把数据进行预留锁定，预留锁定成功后执行本地事务
     * 然后执行confirm阶段，把预留锁定释放掉，更新到新的业务数据中
     * 如执行cancel阶段，把预留锁定的数据还原之前的业务数据中
   * SAGA： 数据补偿来代替回滚（`用户代码层面`实现数据库回滚，可能会脏写需要加锁）——如阿里的`Seata`完成
     * 把一个大事务分解为可以交错运行的一系列子事务的集合。
     * 每个子事务都有对应的补偿动作（恢复原数据的sql）。
     * 子事务跟补偿动作都是幂等的，他们是满足交换律
     * 执行时在数据库是以提交的方式进行避免数据库锁及死锁的问题，回滚时是通过另外语句进行回滚。所以中间就会导致脏写及脏读问题
   * AT： 数据补偿来代替回滚（在`代理层`根据sql会生成undo log的sql用于回滚，由于是代理生成所以SQL兼容性可能会差，可能会脏写需要加锁）——如阿里的`Seata`完成
       * 与SAGA相同，只是生成的回滚sql是由代理层实现，SQL兼容性差但是对回滚透明

XA事务
```SQL
-- 启动XA事务
XA START 'xxxxxx11111';
UPDATE `server` SET `group` = "ari" WHERE `group` = "a";
-- 结束XA事务分支
XA END 'xxxxxx11111';
-- 准备XA事务
XA PREPARE 'xxxxxx11111';

-- 回滚XA事务
XA ROLLBACK 'xxxxxx11111';
-- 提交XA事务
XA COMMIT 'xxxxxx11111';
```
      
   
异步:(最终一致性)
   * 本地消息表(被动): 本地事务提交后，发送消息到消息表，消息表由`定时任务`轮训去处理消息，处理成功后更新本地事务状态为成功。如果处理失败则更新本地事务状态为失败。
     * 由于需要定时任务去处理消息，延迟性会差一点，而且需要定时任务去轮训查询是否有未处理完的消息。
   * MQ方案(主动): 本地事务提交后，发送消息到`MQ`，MQ接收消息处理成功后更新本地事务状态为成功。如果处理失败则更新本地事务状态为失败。
     * 需要考虑消息丢失的问题及MQ重复消息的问题(可以通过定时任务来弥补消息丢失问题，同时幂等操作可通过分布式锁及数据id/version/status等方式解决)



---
## 分布式数据库的定义
写多读少，事务，高可用，分布式，高并发，海量存储

### 一致性
`事务一致性`
* `ACID` 常规关系型数据库 
  * C一致性是目的  A原子性 C隔离性 D持久性 是手段 ———C隔离性是最难的
  * C隔离性使用MVCC进行,通过undo Log进行回滚查看，事务提交后如果不需要会进行清除
* `BASE` 分布式数据库  
  * BA基本可用性  S数据处理中可能各个节点看到的不一致  E最终一致性

`数据一致性`(多个副本数据一致性)     
  * 强一致：所有的副本都返回成功才算成功 （性能差,可用性差）
  * 弱一致：最终一致性
    1. 写副本A然后读副本B   (B落后A——使用写读同个副本解决)
    2. 读副本B后读副本A   (B落后A——使用同个设备/人同个副本解决)
    3. 写副本A然后写副本B,读副本C时顺序错乱 (两个主，同步的顺序——使用全局时钟解决)


### 分布式数据库的类型·
* `Agent`: 在Agent(协调节点)进行转发，SQL重写,多节点的查询，多节点联表，多节点事务，多节点数据汇总排序等
* `NewSQL`: 原生数据库，基础是NoSQL(分片，Paxos/Raft共识,LSM-Tree等)
  * 支持`动态分片`，可以根据分片的请求压力切换到不同的节点上保证节点之间的负载是均衡的。 同时可以把事务内的控制在同一个节点上。
  * 有单独的`元数据集群`进行保存，同时通过同步协议进行同步
  * 由于是NewSQL重新实现了数据库的内部逻辑，所以在OLTP上进行了扩展`HATP`两个存储结构引擎（混合OLAP分析及OLTP事务系统）——Agent这一点就无法实现只能通过ETL方式
    * OLTP 使用行式存储  （写快，单行读快 多行读慢）
    * OLAP 使用列式存储/全文检索等  （写慢(需要写多个数据页-使用LSM-Tree来优化)，单行读慢，同列多行快）

### 分布式数据库局限
* 不使用存储过程（移植性差，调试难）——数据在多个节点及分片原因存储过程会变得更加复杂(不常用所以产商不深挖)
* 不使用自增主键 (唯一性，单调递增 都不能满足) ——  分布式生成需要考虑中心化分配id问题（高并发性能影响）
  * 雪花算法：     
    1. 局部有序(随机的高位(前51位)使得数据在分片中分布更加均衡，避免尾部效应)
    2. 分散分布式数据库id生成压力(解决单点瓶颈及锁)  
    3. 当使用range分片时会导致数据某个分区压力过大(雪花算法前面的机器码可以分散到其他分片)
       * 数据库定义为BIGINT: 1位符号位(固定为0)  41位时间戳(毫秒)  10位机器码(最大1024个机器)  12位序列号(最大1毫秒生成4096个id)
  * UUID：
    1. 无序性(避免分布式数据库单点压力大的问题)
    2. 索引效率低(单分片下无序导致性能较差)

### 计算与存储分离优化方案（计算节点与存储节点之间没有无效的数据）
* `计算下推`到存储节点
    * TiDB采用“缓存写提交”则将写的SQL缓存起来，直到事务提交再一起发到存储节点。即在同个事务中执行insert后执行select时，insert还没有到存储节点。后续改进为将数据节点跟本地缓存进行merge合并得到真实数据(与刷脏页逻辑相同)
* `分区键`
    * 可以根据分区间下推到各个存储节点，同时多表关联也是使用相同的下推方式。以确保数据移动最少
* `索引`
    * 全局索引： 全局索引是为了检验唯一索引的值是否有冲突(如果去各个分区索引去查找效率较差)，但是维护这个全局索引需要加上分布式锁等代价
    * 分区索引： 索引是为了可以快速的查找对应的数据是否满足而不用遍历全部进行查询，当分区的数据分裂合并时对应的分区索引也要发生改变
* `JOIN下推`
    * 大表关联`小表`
        1. 静态(每次更新时各个节点都处理)：定义为每个节点都有的广播表，执行join时大表所在节点join小表
        2. 动态(有需要时再同步到广播到各个节点)：执行时把小表的数据广播到每个对应的节点，执行join时大表所在节点join小表
    * 大表关联`大表`
        1. 重分布： 在多个节点上做本地关联合并
            * 关联时如果指定了A的分区键，没有B的分区键，那么把B的数据推送到A的各个分片上进行本地管理(明确了A的所在的分区，把B的数据推送到A上面去)
            * 关联时如果都不是AB表的分区键，则将A跟B的数据推送到各个节点（所有节点都有A跟B的数据，所有节点联表后，到统一去重）
* `LSM-Tree`
    *
    * level
* `Raft`

    
### 探讨是是不是分布式数据库
1. 是否分布式了
2. 是否能满足ACID

上云后觉得这些就应该让云厂商去做。让专业的人做效率及准确性更高。目前云厂商给出的方案:
1. shared nothing: 方案最成熟，各个节点处理自己的事，处理跨节点事务
2. raft同步: 需要同步，更新半数同意原则（最贴近分布式去中心化的思想，但是节点越多raft同步的效率越差,该方案也可以去掉proxy层请求可内部转发）
3. 存算分离: 优势扩展时只需增加server节点，不用考虑数据节点

ACID与CAP理论
* 传统数据库应该满足ACID(AID是手段，C是目的)
  * Atomic: 原子性 
  * Isolation: 隔离性
  * Durability: 持久性
  * Consistency: 一致性
* 由于分布式存在多副本的问题那就引出CAP理论，CP:强一致性(关系型数据库)  AP:最终一致性(NOSQL数据库)
  * P分区容忍性: 当部分节点因网络原因而彼此失联系统仍能正确地提供服务的能力
  * C一致性: 任何时刻、任何分布式节点中，我们所看到的数据都是没有矛盾的（因网络原因同步不完成，需等到全部同步完才能提供服务）
  * A可用性: 任何时刻都能访问到分布式中的数据（可以牺牲一致性，因网络原因同步不完成，依旧可以提供服务只是数据时旧数据）

跨库事务的方案
1. proxy方案(适用写多同时有share key情况): 修改sql
   * 广播表（小表），更新是更新到每个库，每次更新都广播到各个实例
   * `涉及到多实例处理时需要改写sql`
   * 查询/更新各个实例的数据在proxy层汇总处理或某台实例汇总处理
   * 一致性读的问题通过GTMS方案解决
2. raft方案(适用查多写少): 
   * 节点会通过raft算法同步其他节点的部分数据(该方案无主备集群。把备数据放到其他节点上)
   * 查询时如果当前节点有全部的数据则可直接查找返回，如果无其他数据可向其他节点获取后再本地合并(temporary table)
   * `更新数据时，多半同意机制（如果小于一半回滚的开销也很大）`
   * 一致性读的问题通过GTMS方案解决
3. 存算分离方案:
   * 所有实现在引擎层处理这个需要看各个云商家的处理
   * 由于是存算分离，那么在server层可能有多个，此时写操作命中A服务，其他BCD..等的buffer_pool可能命中不到


## 腾讯云TDSQL