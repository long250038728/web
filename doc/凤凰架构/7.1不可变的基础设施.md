云原生
    “不可变基础设施”提升与微服务平级的重要程度。让不局限于运维，程序升级和部署的手段。
        而是升华为应用代码隐藏分布式的复杂度，让分布式架构得以普遍推广的前提。
    
    容器，编排系统，服务网格为主线，

容器化
    容器是云计算，微服务的基石。首要目标是让软件分发部署过程从传统的发布安装包，靠人工部署转化为直接发布已经部署好的，
        以到达“一次编译，导出运行”
      
    容器化：操作系统虚拟化的另外一个名字
        容器化是虚拟化的子集，值提供操作系统内核以上部分的abi兼容性和完整性环境的兼容性。
            容器化牺牲了一定的隔离性和兼容性，换来的是更快的启动速度，运行性能和更低的执行负担

    isa：指令集架构（x86 ARM）
    abi，应用二进制接口 (window linux)
    环境兼容： 环境变量，版本号
    
    指令虚拟化：
        通过软件来模拟不同isa架构的处理器过程，将虚拟机发出指令转换为符合本机的isa指令
            能提供几乎不受限制的兼容性。由于每条指令都要考软件来转换和模拟，他也是性能损失最大虚拟化技术
    硬件抽象层虚拟化：
        以软件或直接通过硬件来模拟处理器，芯片，内存，磁盘控制器等设备的工作过程，
    操作系统层虚拟化：
        无论指令虚拟化还是硬件虚拟化，都会运行一套完全真实的操作系统来解决ABI兼容和环境兼容性。
            采用隔离手段，使不同进程拥有独立的系统资源。看起来仿佛独享了整个操作系统一样。单其实内核依然是被不同进程共享
    运行库虚拟化
        选择使用软件的翻译方法来模拟系统，他以独立的进程来替换操作系统提供目标软件的全部能力。
    语言层虚拟化
        由虚拟机将高级语言生成的中间代码转换为目标代码可以直接执行命令。
            虽然提供不同系统下相同接口的标准库，单本质上虚拟化并不直接解决ABI兼容性和环境兼容性


容器
    最初是为了隔离计算机资源，后来演变到软件的部署。
    chroot（隔离文件）
        当某个进程经过chroot操作后，他的根目录会被锁定到命令参数下的指定位置，以他或他的进程将不再访问和操作该目录之外的其他文件
            起到了沙箱的作用。
    namespace（隔离访问操作）
        避免不同开发者提供的api相互冲突，由内核直接提供的全局资源封装，是内核针对进程设计的访问隔离机制
        进程是一个独立的linux空间，会觉得自己就是linux，拥有一切资源，不仅文件系统时独立的，还有独立的pid，网络等
    cgroups（隔离资源）
        避免一个进程出问题影响到其他其他进程，用于隔离或分配并限制资源配合（处理器时间，内存大小，磁盘io等）


容器演进
    lxc(linux containers)
        为了减低虚拟化的使用门槛，发布了linux容器的系统虚拟化功能。
            lxc是一种封装系统的轻量级虚拟机，而docker眼中的容器定义则是封装应用技术。
                如果要建设一个LAMP应用。按lxc的思路需要编写安装LAMP虚拟系统，作为那个时代需要有各个补丁，配置都自己搞定（无现在的镜像）
    docker
        他的能力直接源于lxc 开源后就有人用100行代码实现docker核心功能。对比linux内核他的价值在于
            1.跨机器的绿色部署
                定义了将应用及环境打包到一起的格式。lxc没有提供这个能力
            2.已应用为中心
                封装应用而非封装机器的理念
            3.自动构建
                无需关注机器的具体配置
            4.多版本
                可检查版本间的差异，提交或回滚的操作。从历史记录中可看容器是如何一步步构建的
            5.组件重用
                以基础镜像来构建更专业的镜像

        
    kubernetes
        容器编排框架，令集群得以实现跨数据中心的绿色部署，按实际情况扩容
            前身是管理系统Borg，让复杂的软件在云计算下可以合理的编排及扩容伸缩。它符合了所有云计算大厂的切身利益，所有他的成功是必然的
            kubelet是集群节点的代理程序，负责与管理集群的master通信 


-------------
容器编排
    单体时代过去后，分布式系统不再是同于一个进程，容器之间顺畅的协作是核心需求。
        如何调度，分配资源，扩缩规模，如何最大限度的接管系统中的非功能特性

    如现在有个场景需要nginx和filebeat
        1.如果把他们打包成一个镜像。违背了docker单个容器分装单个进程应用的最佳实践
            dockerfile只运行一个entrypoint，因为只会监听pid为1的进程状态来判断容器是否正常
            如果使用supervisiord之类的进程控制器来同时启动nginx和filebeat，如果他们崩溃重启，docker无法察觉
        2.通过docker-composer启动两个容器，通过挂在目录的方式。（容器中的“进程组”）
            docker提测单进程封装的理念影响下，容器蕴含的隔离性也多了仅针对单个进程的额外局限，
                同一个进程组中的多个进程天然就可以共享相同的访问资源跟资源配额。
                容器中的“进程组”对应kubernetes设计中的pod
                    他们共享：
                        UTS名称空间：主机名或域名
                        网络名称空间：网卡，ip等
                        IPC名称空间：可通过信号量或posix共享内存等通信
                        时间名称空间：共享相同的系统时间
                pod另一个基础的职责就是原子性调度
                    如果容器编排不跨集群，是否具有原子性无所谓，但如果跨机器，那非常重要，已容器为单位调度的话，不同容器有可能分配到不同机器上。他们通信需要依靠网络，这个时候谈名称空间共享，cgroup配额共享都失去意义。


kubernetes 健壮与弹性
    pod可以直接创建，但构成pod系统十分脆弱，正确的做法是通过ReplicaSet来创建pod。
        一旦pod发生奔溃退出，或状态异常，ReplicaSet会自动创建新的pod来代替异常的pod，
        当出现额外数量的pod，ReplicaSet会自动回收掉，确保pod数量向期望状态靠拢。
            ReplicaSet功能：
                1.出现故障自动恢复，确保pod数量在期望状态，支持滚动更新，先停止少量旧副本，新副本提供服务后，再持续停止其他旧的副本，添加新的副本
                2.遇到流量压力，可以手动修改Deployment数量，让kubernetes部署更多的pod副本应对压力
                 也可以无需人工参与，通过提供autoscaling资源和自动扩缩控制
                    （根据指标，处理器，内存，用户自定义的度量值来设置Deployment的期望状态)

    Deployment =》 ReplicaSet =》 pod

-------------
kubernets工具

以应用为中心的封装
namespaces，cgroups，chroot  => LXC => Docker => kubernetes

kubernetes需要部署一个或多个配置中心，注册中心，服务网关，安全认证，各个方服务。
为每一个微服务配置Devplyment，ConfigMap，statefulSet，HPA，Service，ServiceAcount，Ingress等资源编写元数据配置
这个过程最难的不仅是繁琐，而且还要编写合适的元数据描述文件，需要多方来的来管理，部署，维护，一般企业找不到合适的角色。
（复杂性不是kubernetes带来的，而是分布式本身的原罪）
    懂开发（镜像版本，依赖变量，项目内容），
    又要懂运维的人（要部署多少服务，如何配置扩容策略，数据库秘钥等），
    有时候还要懂平台（如何调度策略，管理集群资源）


-------------
无状态工具

kustomize
“如何封装应用”的解决方案是“用配置文件来配置文件”，针对yaml模板引擎的变体，

    1.kubernetes认为应用是一组具有相同模板的资源的集合，如果逐一管理，部署每一项元数据过于繁琐。
        那就提供一种便捷的方法，把应用中不变的信息和易变的信息分离。
    2.kustomize被纳入kubectl命令中，使用kustomization文件组织与应用相关的资源，
        （根据环境生成不同的部署配置，只要建立多个kustomization文件）
    3.kustomize使用base，overlay和patch生成最终的配置文件的思路与docker中分层镜像的思路有点相似。
        简化产品针对不同情况的重复配置，（但并没有解决应用的复杂性，该写的配置最终么有少，只是无需重复写）
        

Helm
更具系统性的管理和封装应用的解决方案参考了linux发行管理应用的思路
    成为了这个操作系统系统上面的应用商店与包管理工具

    helm模拟就是像yum，提出包管理直接对应的chart格式和responsitory应用仓库。

    chart
        用于封装kubernetes应用涉及的所有资源，通常以目录内的文件集合的形式存在，目录名就是chart名称。
        由于Chart封装足够丰富的信息，helm处理支持命令行操作外，很容易根据这些信息自动生成图形化的应用安装，参数设置界面
            Chart.yaml给出应用自身的详细信息（名称、版本、许可证、自述、说明、图标，等等）
            requirements.yaml给出应用的依赖关系，依赖项指向另一个应用的坐标（名称、版本、Repository 地址）
            values.yaml给出了所有可配置项目的预定义值（他以花括号包裹在templates目录下的资源文件中）
                部署应用时，helm会将管理员设置的值覆盖到values.yaml默认值上，
                    然后以字符串的替换的形式传给templates目录的资源模板，最终生成kubernetes的资源文件
            
    Repository
        用于实现chart的搜索与下载服务。
        helm维护了公开的stable和incubator的中央仓库。也支持其他人或组织搭建的私有仓库和公共仓库。并通过hub服务把不同个人或组织搭建的功能仓库聚合起来。
            形成更大些的分布式应用仓库，有利于chart的查找与分享

helm提供应用全生产周期、版本，依赖项的管理，同时支持扩展差点，已模仿包管理的方式管理kubernetes应用，但在同一个应用部署多分副本才是常规，helm为进行同一个chart包进行多次部署，每次安装应用都产生一个版本，对于无状态的服务，helm依靠不同版本可以支持多个服务并行工作，但有状态的服务下，helm无法很好的管理这种状态的依赖关系，

------------
有状态工具

Operator与CRD
Operator不应当成为工具或系统，他应该算一种封装，部署和管理kubernetes的应用方法。尤其针对有状态的应用进行封装运维能力的解决方案
    Operator是通过Kubernetes1.7开始支持的自定义资源CRD，把应用封装成另一种更高高层次的资源。
        把kubernetes的控制器模式从面向内置资源扩展到面向所有自定义资源。以完成复杂应用的管理。

    将高级指令转换为低级操作
        有状态应用
            缓存，数据库，消息队列等
        无状态应用 
            web服务器
    
    在kubernetes角度，是否有状态的差异在于会对某些外部资源有绑定性的依赖
        如elasticserch建立实例必须依赖特定的存储位置，重启后依然指向同一个数据文件才能认为是相同的实例

kubernetes从1.9版本发布statusfulSet对应的StatefulSetController。与普通的ReplicaSet中的Pod相比，有几个额外特性
    1.pod会按顺序的创建和销毁
        statefulSet中各个Pod会顺序创建，必须保证前面的pod转入就绪状态才会继续。
    2.pod具有稳定的网络名称
        pod都有唯一的名称，在普通副本中尝试的是随机。在statefulSet为带有顺序编号的名称，重启后依旧不变
    3.有稳定的持久化存储
        每个pod都有独立的PersistentVolumeClaim资源，即使Pod被重新调度到其它节点上，它所拥有的持久磁盘也依然会被挂载到该Pod


OAM开发应用模型
    阿里云和微软发布，核心是如何将开发人员，运维人员与平台人员关注点分离，把云原生定义为一组相互互联但又离散独立的组件组成。实例化在合适的运行时上，由配置来控制行为并共同协作提供统一的功能
        开发人员关注业务逻辑实现
        运维人员关注程序平稳运行
        平台人员关注基础设施能力与稳定性

    一个Application由一组components（服务组件）构成，每个component的运行状态由workload（工作负荷）描述，每个component可以施加traits（运维特征）来获取运维能力，同时我们可用Application Scopes（应用边界）将Components划分到一个或多个应用边界中，便于统一配置，限制，管理。
