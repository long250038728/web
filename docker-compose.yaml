version: '3.8'

services:
  consul:
    image: consul:1.15
    container_name: consul
    networks:
      my-service-network:
        ipv4_address: 172.40.0.11
    ports:
      - "8500:8500"
    command: agent -dev -ui -client='0.0.0.0'
    restart: always

  etcd:
    image: bitnami/etcd:latest
    container_name: etcd
    networks:
      my-service-network:
        ipv4_address: 172.40.0.12
    ports:
      - "2379:2379"
      - "2380:2380"
    environment:
      ALLOW_NONE_AUTHENTICATION: "yes"
    restart: always

  kong-database:
    image: postgres
    container_name: kong-database
    networks:
      my-service-network:
        ipv4_address: 172.40.0.21
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: kong
      POSTGRES_DB: kong
      POSTGRES_PASSWORD: kong
    volumes:
      - pg-data:/var/lib/postgresql/data
    restart: always
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U kong" ]  # 检查PostgreSQL连接状态
      interval: 10s
      timeout: 5s
      retries: 3

  kong-migrations:
    image: kong
    depends_on:
      kong-database:
        condition: service_healthy
    networks:
      - my-service-network
    environment:
      KONG_DATABASE: postgres
      KONG_PG_HOST: kong-database
      KONG_PG_USER: kong
      KONG_PG_PASSWORD: kong
    command: kong migrations bootstrap
    restart: on-failure

  kong:
    image: kong
    container_name: kong
    depends_on:
      kong-migrations:
        condition: service_completed_successfully
      consul:
        condition: service_started
    networks:
      my-service-network:
        ipv4_address: 172.40.0.22
    ports:
      - "8000:8000"
      - "9080:9080"
      - "8443:8443"
      - "8001:8001"
      - "8444:8444"
    environment:
      KONG_DATABASE: postgres
      KONG_PG_HOST: 172.40.0.21
      KONG_PG_USER: kong
      KONG_PG_PASSWORD: kong
      KONG_PROXY_ACCESS_LOG: /dev/stdout
      KONG_ADMIN_ACCESS_LOG: /dev/stdout
      KONG_PROXY_ERROR_LOG: /dev/stderr
      KONG_ADMIN_ERROR_LOG: /dev/stderr
      KONG_DNS_RESOLVER: 172.40.0.11:8600
      KONG_DNS_ORDER: SRV,LAST,A,CNAME
      KONG_ADMIN_LISTEN: 0.0.0.0:8001, 0.0.0.0:8444 ssl
      KONG_PROXY_LISTEN: 0.0.0.0:8000, 0.0.0.0:9080 http2, 0.0.0.0:9081 http2 ssl
    restart: always

  konga:
    image: pantsel/konga
    container_name: konga
    networks:
      my-service-network:
        ipv4_address: 172.40.0.23
    ports:
      - "1337:1337"
    depends_on:
      kong:
        condition: service_healthy
    restart: always

  mysql:
    image: mysql:8.0
    container_name: mysql
    networks:
      my-service-network:
        ipv4_address: 172.40.0.31
    ports:
      - "3306:3306"
    environment:
      MYSQL_ROOT_PASSWORD: root123456
    volumes:
      - mysql-data:/var/lib/mysql
    restart: always
    healthcheck:
      test: [ "CMD", "mysqladmin", "ping", "-h", "localhost" ]  # 检查MySQL是否存活
      interval: 10s  # 每10秒检查一次
      timeout: 5s    # 单次检查超时时间
      retries: 5     # 失败重试5次后标记为不健康
      start_period: 30s  # 容器启动后30秒开始检查

  redis:
    image: redis:7.2
    container_name: redis
    networks:
      my-service-network:
        ipv4_address: 172.40.0.32
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
      - ./redis/conf/redis.conf:/usr/local/etc/redis/redis.conf
    command: >
      redis-server /usr/local/etc/redis/redis.conf 
      --requirepass root123456
    restart: always
    healthcheck:
      test: [ "CMD", "redis-cli", "-a", "root123456", "ping" ]
      interval: 10s
      timeout: 5s
      retries: 5

  zookeeper:
    image: bitnami/zookeeper:3.7
    container_name: zookeeper
    networks:
      my-service-network:
        ipv4_address: 172.40.0.33
    ports:
      - "2181:2181"
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    volumes:
      - zookeeper_data:/bitnami/zookeeper


  kafka:
    image: bitnami/kafka:2.8
    container_name: kafka
    networks:
      my-service-network:
        ipv4_address: 172.40.0.34
    ports:
      - "9092:9092"
      - "9093:9093"
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=172.40.0.33:2181
      - KAFKA_CFG_LISTENERS=INTERNAL://:9092,EXTERNAL://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://172.40.0.34:9092,EXTERNAL://172.40.0.34:9093
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=INTERNAL
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
      - ALLOW_PLAINTEXT_LISTENER=yes
    volumes:
      - kafka_data:/bitnami/kafka
    depends_on:
      - zookeeper
    healthcheck:
      test: [ "CMD-SHELL", "kafka-broker-api-versions.sh --bootstrap-server localhost:9092 || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 6
      start_period: 40s

  canal-server:
    image: canal/canal-server:latest
    container_name: canal-server
    networks:
      my-service-network:
        ipv4_address: 172.40.0.41
    ports:
      - "11111:11111"
    environment:
      canal.instance.master.address: "172.40.0.31:3306"
      canal.instance.dbUsername: "root"
      canal.instance.dbPassword: "root123456"
      canal.mq.topic: "canal"
      canal.serverMode: "kafka"
      canal.mq.flatMessage: "true"
      kafka.bootstrap.servers: "172.40.0.34:9093"
    depends_on:
      mysql:
        condition: service_healthy
      kafka:
        condition: service_healthy
    restart: always

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    networks:
      my-service-network:
        ipv4_address: 172.40.0.42
    ports:
      - "8080:8080"
    environment:
      - KAFKA_CLUSTERS_0_NAME=local
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=172.40.0.34:9092

networks:
  my-service-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.40.0.0/24

volumes:
  pg-data:
  mysql-data:
  redis-data:
  kafka_data:
  zookeeper_data: